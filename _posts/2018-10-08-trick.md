---
layout: post
title: Machine learning in 20 minutes
tags: math, AI
---

Anybody can be a machine learning engineer.  All it takes is time and perseverance.  But who has time now days?  Moreover, what if you're lazy? Or both?!  Then you've come to the right place. The goal of this post is to teach you just enough machine learning so that you can fake it until you make it.

## Problem Setup
Let's say you have a table of the form

| X | Y |
|---|---|
|5 | 0 |
|9 | 0 |
|1 | 1 |
|10| 0 |
| ... | ... |
|-2| 1 |
|39|0 |
|-5| NaN |
|0 | NaN |
|23| NaN
|...|...|

We'd like to fill in those `NaN`s with actual values.  Could we possible infer them.  Is this even possible?

### A minimal assumption
The only missing thing seems to be the $$y$$'s. So perhaps we could find a function from $$f(x)$$ that takes in $$x$$ and guesses a candidate $$\hat{y} = f(x)$$.

If such a function exists then if we were to filter just the readings where $$x=0$$ (i.e. SQL query `SELECT * FROM table WHERE x = 0`) we should get a table that looks like

| X | Y |
|---|---|
| 0 | 1 |
| 0 | 1 |
| 0 | 1 |
|...|...|
| 0 | 1 |

where the $$y$$-value is always the same (perhaps 0).
This is a big assumption.  In fact, if such an $$f$$ exists, then this property must also hold for $$x=1,2,...$$, basically any $$x$$-value we expect to encounter. If this is the case we say that $$y$$ depends deterministically upon $$x$$.

Deterministic dependence is not a realistic assumption in most contexts.  More generally, you'd desire that $$f(x)=y$$ most of the time.  This is only possible if the link between $$x$$ and $$y$$ is stronger than a coin flip.

Even more generally, what we really need for machine learning to be worth while is that there be a statistical relationship between $$x$$ and $$y$$. Statistically speaking, the minimal assumption is that $$y$$ depends on $$x$$ (i.e $$\Pr(x,y) != \Pr(x) \Pr(y)$$ for at least some $$x$$).

### The most naive solution
If the first assumption holds and $$y$$ depends deterministically on $$x$$ then the most naive solution is to let $$f(x)$$ be a simple query of the table.

If the dependence is statistical, the most naive solution is to define the prior
$$
\begin{align}
  \Pr(Y=y | X=x) := \frac{\text{COUNT * FROM table WHERE X=x AND Y=y}}{\text{COUNT * FROM table WHERE X=x}}
\end{align}
$$

Then we define the decision function
$$
\begin{align}
  f(x) = \begin{cases}
    1 &\text{ if } \Pr(Y=1 | X=x) > 0.5 \\
    0 &\text{ else }
    \end{cases}.
\end{align}
$$

In some sense, all machine algorithms are a variant of this one.  Machine learning is nothing but the field of filling in blanks in tables by statistically memorizing the non-blank portion of the table.

One of the strengths of this solution is that no matter what the relationship between $$y$$ and $$x$$, this method will capture it. The other strength is that it is conceptually simple. The issue with this specific solution is that it's not really scalable, it gets harder to deal with the bigger the table is, and assumes we have an $$(x,y)$$ pair for every $$x$$ that could possibly occur.  This might not be so bad if $$x$$ is a single integer ranging from $$-1000$$ to $$1000$$, however, if the $$x$$'s consisted of a tuple of $$57$$ integers each ranging from $$1$$ to $$10$$, then there are $$10^{57}$$ possible values for $$x$$, which is impossible to store even if we etched a value onto every atom in the universe.

Another weakness is that this method is prone to **over-fitting**, a concept which will be elaborated on later.

### A less naive solution
To make a solution that is scalable, a sensible thing is to define the posterior with an unknown parameter. In this way, the complexity of the method is defined by a chosen parameter rather than the size of the dataset. We then can tune the parameter until we are happy with the results. A standard example of this is called logistic regression, where we use the [logistic function](https://en.wikipedia.org/wiki/Logistic_function) to define the posterior

$$
\begin{align}
  \Pr(Y=1 | X=x, \beta) := {\rm logistic}( \beta_1 x + \beta_0)
\end{align}
$$

parametrized by $$\beta = (\beta_0, \beta_1) \in \mathbb{R}^2$$. As before, we can then define our decision function by

$$
\begin{align}
  f(x; \beta) = \begin{cases}
    0 & \text{ if } \Pr(Y=1 | X=x, \beta) < 0.5 \\
    1 & \text{ else }
  \end{cases}
\end{align}
$$

The process of determining $$\beta$$ is called **training**.

### Training
The basic idea of training is to define a cost function and choose the parameters $$\beta = (\beta_0,\beta_1)$$ which minimize the cost. The cost function is really a function of $$\beta$$ as well as the dataset, which we will just call $$\mathcal{D} = \{ (x_i,y_i) \}_{i=0}^{N}$$. For example, the most naive cost function is to just use the (negative) accuracy.  Accuracy is nothing but the portion of times your classifier got the answer correct.

$$
\begin{align}
  \text{acc}(\beta,\mathcal{D}) = \frac{\| \{f(x;\beta)=y \mid (x,y) \in \mathcal{D}\} \| }{\|\mathcal{D}\|}
\end{align}
$$

Then you would set $$\beta := \arg\max_{\hat{\beta}}(\text{acc}(\hat{\beta},\mathcal{D}))
$$. However, solving for $$\beta$$ is difficult here because accuracy is a discontinuous function of $$\beta$$. **There are no algorithms (except naive search) for finding the minima of a non-differentiable function.**

A more standard cost function is to use the negative log likelihood function

$$
\begin{align}
  C(\beta, \mathcal{D}) = -\sum_{i}\log(\Pr(Y=y_i | \mid X=x_i, \beta))
\end{align}
$$

The standard one used for the Logistic regressor is the cross-entropy

$$
\begin{align}
  \text{xent}(\beta,\mathcal{D}) = \sum_{i} y_i \log(\Pr(Y=1| X=x_i; \beta)) + (1-y_i) \log( \Pr(Y=0 \mid X=x_i; \beta))
\end{align}
$$

which is smooth respect to $$\beta$$. To see that this is a good idea, just note that if $$y_0=1$$ then we gain the most from the $$0$$th term when $$\Pr(Y=1 \mid X=x_0)$$ is large.  Conversely, if $$y_0=0$$ then the converse holds.

The other big advantage of cross-entropy is theoretical, but important.  Cross entropy has a [statistical interpretation](https://en.wikipedia.org/wiki/Cross_entropy) which meshes with the probabilistic interpretation of logistic regression. This is probably more than you would need to know to trick your interviewer into hiring you though.

### Overfit
Let's say your data looks like this

![dataset]({{"assets/trick/dataset.png" | absolute_url }})

A logistic regressor would not be too shabby here.  The optimal logistic regressor would do something like this

![plot_of_logistic_regressor]({{"assets/trick/dataset_and_predictor.png" | absolute_url }})

We see it misses one of the data points (around $$x=0.6$$). However, our naive classifier might do something like this

![plot_of_overlayed_solution_naive]({{"assets/trick/dataset_and_naive.png" | absolute_url }})

So naively speaking, the naive classifier is better, right?  Wrong!  If I were to cover up that point at $$x=0.6$$, and ask you to just guess the value, using raw intuition what would you say?  You'd probably agree with the logistic regressor and say $$y=1$$. This over-confidence in the "optimal" solution will come back to bite you when this classifier is released into the wild, encounter's an instance of $$x=0.6$$ and emphatically says $$y=0$$, and the world responds "what?  Are you an idiot?  Look around yourself! $$y=1$$!". It's more likely there is a glitch in our data, and we are just memorizing the glitch.

### Bias
The converse problem to overfit is called **bias**. This is where the family of models is not powerful enough to capture the patterns in the dataset.  For example, if the data looked like this

![dataset]({{"assets/trick/dataset2.png" | absolute_url }})

then a logistic regressor would be suffering from high **bias** because it can only split the data into two segments. We'd get a fit that looks like

![logistic2]({{"assets/trick/logistic_2.png" | absolute_url }})

All those points where $$x > 0.5$$ would be mis-classified. We say the logistic regressor is suffering from high bias.  To solve this we should add some expressive power to the model.  We need more levers to pull. If we considered a higher-order model such as

$$
\begin{align}
  \Pr(Y=1 \mid X=x) = {\rm logistic}( \beta_2 x^2 + \beta_1 x + \beta_0)
\end{align}
$$

we could get this plot

![logistic2_quad]({{"assets/trick/logistic_2_quad.png" | absolute_url }})

which is much more satisfying.

### Protecting yourself from bias and over-fit
Overfit really only becomes apparent when you fit your model, and then throw some new data at it, that it has never seen before. The way you do this in practice is to split your labeled data into two components, $$\mathcal{D} = \mathcal{D}_{test} \cup \mathcal{D}_{train}$$. Then you use the training set to train a model, and evaluate the performance on both the training set and the test set.  If the performance is great on the training set but poor on the test set, you are suffering from over-fit.  If the performance is poor on both then it is possible you are suffering from high bias, and you need to choose a more expressive model.  Bias is a bit more subtle to diagnose, because it's also possible that your problem is simply difficult and the dependency between $$x$$ and $$y$$ is weak.

### The bias variance trade-off
Bias can be quantified by the error of your model on the training set. Overfit is often quantified by a precise mathematical quantity called **variance** which measures the difference in performance on the training vs the test set. The most important thing to know regarding bias and variance is that they attack you in different places.  When you have a model with low expressibility you risk high-bias.  As you add expressibility into your model (adding more parameters to play with) you get closer and closer to a naive classifier that merely memorizes your training data.  You merely memorize your training data, and you totally bomb at test time. This is high variance.  At this point I'm obligated to show you this plot

![bias_variance](https://i.stack.imgur.com/GEJIM.png)

## Testing
There are some standard metrics for measuring the performance of classifier

 - **Precision**: $$TP/(TP+FP)$$.  This is high if whenever you shoot, the target deserved to die. This low when you have a lot of false positives and thus collateral damage.
 - **Recall**: $$TP/(TP+FN)$$. This is high if whenever your target must die, you actually shot him/her.  This is low when you have a lot of false negatives and the bad guys get away.
 - **Accuracy**: This is just the proportion of answers you get correct, the same way your teacher would score your tests.  This is a silly metric when your data is unbalanced. For example, in classifying "hot-dog"/"not hot-dog", you could classify all pictures on the internet as "Not hot-dog" and obtain 99.99% accuracy (but you'd score 0 on recall).
 - **$$F_1$$-score**: This is the geometric mean of precision and recall.  Useful if you can't decide what is more important to you.
 - **AUC curve**:  This is hard to explain in a small space.  [See the wikipedia article](https://en.wikipedia.org/wiki/Receiver_operating_characteristic).

## Other words you should know
 - **regressor**: A regressor is just a classifier that predicts into an infinite class, such as the space of real-numbers.
 - **supervised learning**: This is just the process of learning to predict $$y$$ from $$x$$ given a table of $$(x,y)$$ pairs.
 - **unsupervised learning algorithm**:  These algorithms do not predict anything.  They typically just encode data into more useful formats and the encoding method is "learned" from an unlabeled dataset (thus unsupervised).  It's just learning how to encode the $$x$$'s.
 - **Reinforcement learning**:  This learning how to train an algorithm at runtime while in the field (very dangerous).  This is only useful if your $$(x,y)$$ pairs are arriving in a stream and you want to use this incoming data to re-train.  The biggest success stories here come from scenarios where you have an automatic labeller, such as when playing go, the rule of go dictate the winner of the game, and thus Alpha-go was solved using reinforcement learning.

## Final thoughts
If you actually stumbled upon this blog post, and you are hoping to trick some potential employers.  I strongly urge you **not** to do that.  In fact, this post is actually intended to protect employers.
