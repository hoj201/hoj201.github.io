---
layout: post
title: Convolutional Neural Networks in a Nutshell
tags: math, AI
---
There are a lot of posts out there explaining convolutional neural networks with titles like ["An Intuitive Explanation of Convolutional Neural Networks"](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/). These posts are probably good for people who are afraid of equations, because they are almost devoid of them, and use pictures and shit to substitute for the loss.  However, if you are comfortable with equations, such posts do the opposite of what you are hoping for.  They spend 20 minutes explaining the "hard" part, which is actually the easy part for us mathy types, and then they spend virtually no time explaining the "easy" part, which is really the hard part for the mathy types (at least a portion of the  hard part).  This post is addressed to my fellow the mathy types.

## What are convolutional neural networks?
Convolutional neural networks (CNNs) are the go-to method for image classification tasks. For example, detecting if a picture contains a hot dog. We can also use convolutional neural networks to look for signals in a time-series by viewing the time-series as a one-dimensional image. **A CNN is nothing but an artificial neural network where a convolution[^1] is performed at some point**.  Recall, a given a signal $$x(t)$$ for $$t=0,\dots,N$$ a convolution with respect to a kernel $$K(s)$$ for $$s=0,\dots,W$$ is the signal

$$\begin{align}
  y(t) := (K * x)(t) = \sum_{s} K(s) x(t-s).
\end{align}$$

[^1]: Most neural network libraries such as tensor flow replace convolution with autocorrelation. The difference is truly immaterial since it's a matter of index shuffling.  Confusingly, these libraries and papers in the ML community continue to use the phrase "convolution" for both, cuz they suck.

This is a one-dimensional convolution.  The higher dimensional generalization is obvious. It's the same formula but the indices are interpreted as multi-indices. In this post we will focus on one-dimensional CNN's, since the $$N$$-dimensional generalization can be easily extrapolated from a solid understanding of the one-dimensional case.

Now that you have the abstract definition of a CNN, let's get concrete.

## A single layer feed-forward CNN
Let's say you wanted to detect the following shape your signal
![image_of_kernel](url)
Let's call this shape $$k(s)$$.

In the signal below, dubbed $$x(t)$$, we'd say this shape is detected in two locations
![image_of_signal_with_arrows](url)

Now something to note, if you look at the signal $$y(t) = (K*x)(t)$$ we get this plot
![image_of_convolved_signal](url)

Note that the humps occur where the detections should occur. So we can cobble a detector by reporting when $$y(t)$$ exceeds some threshold.  In particular, lets say we want a function that outputs a confidence (i.e. a number between 0 and 1) that this shape is present at each time. Mathematically this could be done by passing $(K*x)(t)$ to the logistic function

$$
\begin{align}
  {\rm logit}(x) &= (1.0 + \exp(-x))^{-1}
\end{align}
$$
which simply squashes $\mathbb{R}$ into the range $$(0,1)$$. This is depicted in the plot of the logistic function below

![logistic plot](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)

So putting all the pieces together, if we let $$Y_t$$ denote whether or not the shape is present at time $$t$$, then our detector is the posterior probability
$$
\begin{align}
  \Pr(Y_t | x) := {\rm logit}( [K*x](t) )
\end{align}
$$

### Further standard customizations
Some critiques of the above formula.  If the signal $$[K*x](t)$$ tends to be very large in magnitude this might behave badly.  For example, say $$[K*x](t)$$ is 10000000 when the shape is present at time $$t$$, but due to random noise, the value of $$[K*x](t)$$ is often in the 100s when the shape is **not present**. Then our detector will certainly detect the shape when it is present, but it will also detect it with unrealistically high confidence when there is simply noise in the signal because $${\rm logit}(100) \approx 1$$.  Thus one should normalize the signal.  This is done by considering instead the posterior

$$
\begin{align}
  \Pr(Y_t | x) := {\rm logit}( \beta_1 \cdot [K*x](t) + \beta_0 )
\end{align}
$$

Furthermore, it is often white to prevent an algorithm form being to certain of itself and outputting a zero[^2].  We can do this by considering the function

$$
\begin{align}
  \Pr(Y_t | x) :=
    \begin{cases}
      {\rm logit}(\beta_0) \text{ if } \beta_1 \cdot [K*x](t) + \beta_0 > 0 \\
      {\rm logit}( \beta_1 \cdot [K*x](t) + \beta_0 ) \text{ else}
    \end{cases}
\end{align}
$$

Alternatively, we can use the function $${\rm ReLU}(x) := \max(0,x)$$ and just write this as

$$
\begin{align}
  \Pr(Y_t | x) := {\rm logit}( {\rm ReLU}(\beta_1 \cdot [K*x](t) + \beta_0 ) )
\end{align}
$$

The function $${\rm ReLU}$$ is called the "rectified linear unit".  It's pretty common in deep learning and used for "introducting non-linearity" into a classifiers (as if linearity is a bad thing. I know... until you start building classifiers yourself it sounds super weird).

[^2]:The justification for this is well known to people who've implemented a Naive-Bayes classifier.  Let say you have a Naive Baye's classfier $$\Pr(Y | X) := \Pr(Y | X_1) \Pr(Y|X_2) \cdots \Pr(Y|X_n)$$.  Then if and of $$\Pr(Y | X_1), \dots, \Pr(Y | X_n)$$ vanish, the whole classifier outputs a $$0$$, regardless of a high confidence for the others.  That tyranny of a minority is dangerous.

Lastly, there might be noise in the signal's magnitude (vertical noise if you will), and there might also be noise with respect to time (horizontal noise if you will). To mitigate horizontal noise, we should find a good way to down-sample.  The standard choice in this community is to use max-pooling.  This is where you just partition your signal into bins and take the max of each bin to get a downsampled signal. This is depicted for two-dimensional signals at this picture I stole from [here](https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png)

![maxpool 2d](https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png)

So the most generically common single layer CNN one could build takes the form

$$
\begin{align}
  \Pr(Y_t | x) := {\rm logit}( {\rm maxpool}_w(\beta_1 {\rm ReLU}([K*x]+\beta_0)(wt) )
\end{align}
$$
where $$w$$ is the size of the bins in the max-pooling operation.

## Deep CNNs
What was built in the last section was a single layer feed forward CNN.  A deep CNN does what the single layer does, but iterates on the output a few more times, with different kernels.  Typically, the logistic function would only be used at the end, and the intermediate steps are just convolutions and rectified linear units.

Let me setup some useful notation. If we have a kernel $$K$$, and a series $$x$$, the expression

$$
\begin{align}
  f_1(x) := {\rm maxpool}_{w_1}(\beta_{1,1} {\rm ReLU}( [K_1 * x])+\beta_{0,1})
\end{align}
$$

is itself a series.  We can then make another function $$f_2$$ with another kernel $$K_2$$ that takes in the series $$f_1(x)$$ as an input and outputs another series, ... then $$f_3$$, you get the point. As a formula we could iteratively define the functions

$$
  \begin{align}
  f_n(x) := {\rm maxpool}_{w_n}(\beta_{1,n} {\rm ReLU}([K_{n} * f_{n-1}(x)]) + \beta_{0,n})
  \end{align}
$$

for $$n=1,\dots,N$$, and let $$f_0(x) = x$$.
The function $$f_N(x)$$ is an instance of a $$N$-layered CNN.

## Why would you want multiple layers


## Sequence to class model
In many cases we are not interested in outputing a series of values $$Y_t$$.  We merely want a single label $$Y$$.  The "standard" here is to pass the sequence into a dense neural network where the output is one-hot encoded labels.

## How do you train a CNN?

## Conclusion
