---
layout: post
title: How to trick somebody into hiring you as a machine learning engineer
tags: math, AI
---

Anybody can be a machine learning engineer.  All it takes is time.  But what if you don't have time?  Or what if you do, but your just lazy?  Then you've come to the right place.
The goal of this post is to teach you just enough machine learning so that you can convince an interviewer who does not know any, that you know a lot.

## Problem Setup
Let's say you have a table of the form

| x | y |
|---|---|
|5 | 0 |
|9 | 0 |
|1 | 1 |
|10| 0 |
| ... | ... |
|-2| 1 |
|39|0 |
|-5| NaN |
|0 | NaN |
|23| NaN
|...|...|

We'd like to fill in those `NaN`s with actual values.  Could we possible infer them.  Is this even possible?

### A minimal assumption
The only missing thing seems to be the `y`s. So perhaps we could find a function from $$f(x)$$ that takes in $$x$$ and guesses a candidate $$\hat{y} = f(x)$$.

If such a function exists then if we were to filter just the readings where $$x=0$$ (i.e. SQL query `SELECT * FROM table WHERE x = 0`) we should get a table that looks like

| x | y |
|---|---|
| 0 | 1 |
| 0 | 1 |
| 0 | 1 |
|...|...|
| 0 | 1 |

where the `y`-value is always the same (perhaps `0`).
This is a big assumption.  In fact, if such an $$f$$ exists, then this property must also hold for $$x=1,2,...$ any integer. If this is the case we say that $$y$$ depends deterministically upon $$x$$.

Deterministic dependence is not really a realistic assumption in most contexts.  More generally, you'd desire that $$f(x)=y$$ for most of the time.  This is only possible if the tie between $$x$$ and $$y$$ is stronger than a coin flip.

Even more generally, what we really need for machine learning to be worth while is that there be a statistical relationship between $$x$$ and $$y$$. Statistically speaking, the minimal assumption is that $$y$$ depends on $$x$$ (i.e $$\Pr(x,y) != \Pr(x) \Pr(y)$$ for at least some $$x$$).

### The most naive solution
If the first assumption holds and $$y$$ depends deterministically on $$x$$ then the most naive solution is to let $$f(x)$$ be a simple query of the table.

If the dependence is statistical, the most naive solution is to define the prior
$$
\begin{align}
  \Pr(Y=y | X=x) := \frac{\text{COUNT * FROM table WHERE X=x AND Y=y}}{\text{COUNT * FROM table WHERE X=x}}
\end{align}
$$

Then we define the decision function
$$
\begin{align}
  f(x) = \begin{cases}
    1 &\text{ if } \Pr(Y=1 | X=x) > 0.5 \\
    0 &\text{ else }
    \end{cases}.
\end{align}
$$

In some sense, all machine algorithms are a variant of this one.  Machine learning is nothing but the field of filling in blanks in tables by statistically memorizing the non-blank portion of the table.

One of the strengths of this solution is that no matter what the relationship between $$y$$ and $$x$$, this method will capture it. The other strength is that it is conceptually simple. The issue with this specific solution is that it's not really scalable, it gets harder to deal with the bigger the table is, and assumes we have an $$(x,y)$$ pair for every $$x$$ that could possibly occur.  This might not be so bad if $$x$$ is a single integer ranging from $$-1000$$ to $$1000$$, however, if the $$x$$'s consisted of a tuple of $$57$$ integers each ranging from $$1$$ to $$10$$, then there are $$10^{57}$$ possible values for $$x$$, which is impossible to store even if we etched a value onto every atom in the universe.

Another weakness is that this method is prone to over-fitting.  This will be elaborated in a later section.

### A less naive solution
To make this scalable, a sensible thing is to define the posterior with an unknown parameter (so the complexity is bounded), and then to tune the parameter until you are happy with the results. A standard example is called logistic regression

$$
\begin{align}
  \Pr(Y=1 | X=x, \beta) := {\rm logistic}( \beta_1 x + \beta_0)
\end{align}
$$

where

$$
\begin{align}
  {\rm logistic}(s) = (1+\exp(-s))^{-1}
\end{align}
$$

and $$\beta = (\beta_0, \beta_1)$$ are yet-to-be-determined parameters. The process of determining these parameters is called **training**.

### Training
The basic idea of training is to define a cost function and choose the parameters $$\beta = (\beta_0,\beta_1)$$ which minimize the cost. The cost function is really a function of $$\beta$$ as well as the dataset, which we will just call $$\mathcal{D} = \{ (x_i,y_i) \}_{i=0}^{N}$$. For example, the most naive cost function is to just use the accuracy

$$
\begin{align}
  \text{acc}(\beta,\mathcal{D}) = \frac{\#\{f(x;\beta)=y \mid (x,y) \in \mathcal{D}\} }{\#(\mathcal{D}}
\end{align}
$$

and then you set $$\beta := \arg\min_{\hat{\beta}}(\text{acc}(\hat{\beta},\mathcal{D}))
$$. However, solving for $$\beta$$ is difficult because accuracy is a discontinuous function of $$\beta$$ and there are no algorithms (except naive search) for finding the minima of a non-smooth function.

A more standard cost function is to use the negative log likelihood function

$$
\begin{align}
  C(\beta, \mathcal{D}) = -\sum_{i}\log(\Pr(Y=y_i | \mid X=x_i, \beta))
\end{align}
$$

The standard one used for the Logistic regressor is the cross-entropy

$$
\begin{align}
  \text{xent}(\beta,\mathcal{D}) = \sum_{i} y_i \log(\Pr(Y=1| X=x_i; \beta)) + (1-y_i) \log( \Pr(Y=0 \mid X=x_i; \beta))
\end{align}
$$

which is smooth respect to $$\beta$$. To see that this is a good idea, just note that if $$y_0=1$$ then we gain the most from the $$0$$th term when $$\Pr(Y=1 \mid X=x_0)$$ is large.  Conversely, if $$y_0=0$$ then the converse holds.

The other advantage of cross-entropy is that is more theoretical, but important.  Cross entropy has a [statistical interpretation](https://en.wikipedia.org/wiki/Cross_entropy) which meshes with the probabilistic interpretation of logistic regression. This is probably more than you would need to know to trick your interviewer into hiring you.

### Bias and Overfit
Let's say your data looks like this

[plot_with_a_single_outlier]()

A logistic regressor is only capable of dividing space into two pieces.  That's okay in this scenario because the optimal solution would do something like this

[plot_of_overlayed_solution_of_logistic_regressor]()

However, our naive classifier might do something like this

[plot_of_overlayed_solution_naive]()

So naively speaking, the naive classifier is better, right?  Wrong!  Just intuitively, it seems more likely by visual inspection that there is a glitch in the data at that one point.  This will become apparent once your algorithm runs into a sample at $$X=10$$, and low and behold, the correct answer is that of the logistic regressor.  This is an example of over-fit.

The converse problem to overfit is called **bias**. This is wear the family of models is not power-ful enough to capture the patterns in the dataset.  For example, if the data looked like this

[plot_of_data](requires_two_dividers)

then a logistic regressor would be suffering from high **bias** because it can only split the data into one segment, not two.

### The bias variance trade-off

### Protecting yourself from bias and over-fit
Overfit really only becomes apparent when you fit your model, and then throw some new data at it, that it's never seen before. The way you do this in practice is to split your labeled data into two components, $$\mathcal{D} = \mathcal{D}_{test} \cup \mathcal{D}_{train}$$. Then you use the training set to train a model, and test the performance on a test set and the training set.  If the performance is great on the training set but poor on the test set, you are suffering from over-fit.  If the performance is poor on both then it is possible you are suffering from high bias, and you need to choose a more expressive model.  Bias is a bit more subtle to diagnose, because it's also possible that your problem is simply difficult and the dependency between $$x$$ and $$y$$ is weak.

## Testing
There are some standard metric for measuring the performance of classifier

 - Precision: This low when you have a lot of false positives.
 - Recall: This is low when you have a lot of false negatives.
 - Accuracy: This is redundant when your data is unbalanced.
 - $$F_1$$-score: This is the geometric mean of precision and recall.
 - AUC curve???  This is something you mention when there is a threshold involved with the classifier and that threshold trades precision for recall.
