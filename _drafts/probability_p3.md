---
layout: post
title: Probabilistic notation is the worst: Part III
subtitle: (now with Machine Learning)
---

<!-- Summary:  measure theory works pretty well with category theory.  We can extend the projection map into a functor to create the category of probability theory.  So first review measure theory.  The objects are cones of measures (define measure), and the arrows are conditional distributions.  Composition is the chain rule.

Baye's Theorem just says that the conditional measures form a groupoid.

PGMs:  It's tempting to relate PGMs because they are directed networks, but this does not quite work. PGMs are the probabilistic analog of computation graphs.  A computation graph is just a graphical representation of a way of computing a function.  So any function has a family of computation graphs. We can embed the space of computation graphs into the space of PGMs by embedding into Dirac measures.
-->
