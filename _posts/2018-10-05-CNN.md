---
layout: post
title: A primer on Convolutional Neural Networks for mathy people
tags: math, AI
---
There are a lot of posts out there explaining convolutional neural networks with titles like ["An Intuitive Explanation of Convolutional Neural Networks"](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/). These posts are probably good for people who are not so crazy about equations, because they are almost devoid of them, and use pictures and shit to substitute for the loss.  However, if you are reasonably comfortable with equations, such posts might do the opposite of what you are hoping for.  They spend most of the mental budget explaining the "hard" part, which is actually the easy part for us mathy types, and this prevents them from going as deep as the equation accepting audience would allow. This post seeks to partially address this.

## What are convolutional neural networks?
Convolutional neural networks (CNNs) are the go-to method for image classification tasks. For example, detecting if a picture contains a hot dog. We can also use convolutional neural networks to look for signals in a time-series by viewing the time-series as a one-dimensional image. **A CNN is nothing but an artificial neural network where a convolution (or cross-correlation) is performed at some point**.  Recall, a given a signal $$x(t)$$ for $$t=0,\dots,N$$ a cross-correlation with respect to a kernel $$K(s)$$ for $$s=0,\dots,W$$ is the signal

$$\begin{align}
  y(t) := (K * x)(t) = \sum_{s} K(s) x(t+s).
\end{align}$$

Just a note on notation, in math and signal processing communities "$$* $$" is used to denote convolution, not cross correlation.  (the $s$ gets replaced with a $$-s$$ in the above formula). However in the ML community, it can go either way. Most neural network libraries such as [tensor flow](https://www.tensorflow.org/) fallaciously call cross-correlation "convolution" and use the \* to denote cross-correlation. It's not the worst error. The difference is mostly immaterial since it's a matter of index shuffling, [...cuz tradition?](https://en.wikipedia.org/wiki/Convolutional_neural_network#Design). I'm just going to continue calling it cross-correlation in this article, because I think it's a stupid tradition, and I'll use \* to denote the process.

Another note: This is a one-dimensional cross-correlation.  The higher dimensional generalization is obvious. It's the same formula but the indices are interpreted as multi-indices. In this post we will focus on one-dimensional CNN's, since the $$N$$-dimensional generalization can be easily extrapolated from a solid understanding of the one-dimensional case.

Now that you have the abstract definition of a CNN, let's get concrete.

## A single layer feed-forward CNN
Let's say you wanted to detect the following shape your signal

![kernel]({{"assets/CNN/kernel.png" | absolute_url }})

Let's call this shape $$K(s)$$.

In the signal below, dubbed $$x(t)$$, we'd say this shape is detected in two locations.  Around $$t=30$$ or $$40$$ and also around $$t=255$$.

![signal]({{"assets/CNN/signal.png" | absolute_url }})

Now something to note, It's tempting to just threshold.  However, there is that annoying spike at $$t=150$$.  So a good way to tease out this shape is to use a filter. If you look at the signal $$\tilde{x}(t) = (K*x)(t)$$ we get this plot

![convolved signal]({{"assets/CNN/convolved_signal.png" | absolute_url }})


Great.  That spike at $$t=150$$ is no longer so big and the spikes at the events of concern are like the twin towers. In particular, the tallest peaks occur where the detections should occur. So we can cobble a detector by checking that $$\max_t(\tilde{x}(t))$$ exceeds some threshold. That's sort of the intuitive first algorithm.  A more nuanced but sophisticated goal would be to translate $$\max_t(\tilde{x}(t))$$ into some measure of confidence (i.e. a number between 0 and 1) that this shape is present in the signal. Mathematically this could be done by passing $$\max_t (\tilde{x}(t))$$ to the logistic function

$$
\begin{align}
  {\rm logit}(z) &= (1.0 + \exp(-z))^{-1}
\end{align}
$$
which simply squashes $$\mathbb{R}$$ into the interval $$(0,1)$$ as depicted in the plot below

![logistic plot](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)

Putting all the pieces together, if we let $$Y$$ denote whether or not the shape is present in the signal, then our detector is the posterior probability

$$
\begin{align}
  \Pr(Y_t | x) := {\rm logit}( \max([K*x](t)) )
\end{align}
$$

### Further embellishments
Some critiques of the above formula.  If $$\tilde{x}(t)$$ tends to be very large in magnitude this might behave badly.  For example, say $$\tilde{x}(t)$$ is 10,000,000 when the shape is present, but due to random noise the value of $$\tilde{x}(t)$$ is often of size 100 when the shape is **not present**. Then our detector will certainly detect the shape when it is present, but it will also detect it with unrealistically high confidence when there is simply noise in the signal because $${\rm logit}(100) \approx 1$$.  Therefore, we should normalize the signal.  This might involve waiting for $$[K * x](t)$$ to exceed some threshold, which means waiting for $$[K*x](t) + \beta > 0$$ for some constant $$\beta_0$$.
Furthermore, it is often wise to prevent an algorithm from being too certain of itself and outputting a zero.[^2]   We can avoid zeros by thresholding and considering the continuous (but non-smooth) function


[^2]:The justification for this is well known to people who've implemented a Naive-Bayes classifier.  See [this stack-exchange posting](https://datascience.stackexchange.com/questions/15526/how-to-handle-a-zero-factor-in-naive-bayes-classifier-calculation).

$$
\begin{align}
  \Pr(Y_t | x) :=
    \begin{cases}
      {\rm logit}(\beta) \text{ if } \max_t([K*x](t)) + \beta > 0 \\
      {\rm logit}( \max_t([K*x](t)) + \beta ) \text{ else}
    \end{cases}
\end{align}
$$

Alternatively, we can re-express $$\Pr(Y_t \mid x)$$ using the function $${\rm ReLU}(x) := \max(0,x)$$ as

$$
\begin{align}
  \Pr(Y_t | x) := {\rm logit}( {\rm ReLU}( \max_t([K*x](t)) + \beta ) )
\end{align}
$$

The function $${\rm ReLU}$$ is called the "rectified linear unit".  It's pretty common in deep learning and used for "introducing non-linearity" into a classifier.[^3]

[^3]:If you've not yet touched some real data, it might sound weird to voluntarily introduce non-linearity.  The main motivation is that if your data exhibits nonlinear relationships, your classifier ought to respect that. At least to some extent.


## How to choose $$K,\beta$$ (training)
Earlier I assumed $$K(s)$$ and $$\beta$$ were given.  Of course this is absurd.  The real task of machine learning is that of systematically searching for these parameters. I won't be able to really address this in the space of a single section of this tiny blog post, but I can give a high level overview.

The process of choosing these parameters is called **training** the neural network.  What you do is you gather a bunch of annotated data. This means pairs $$(x_1,y_1), \dots, (x_n, y_n)$$ where each of the $$x_i$$'s is a sequence, and each $$y_i \in \{ 0,1\}$$ to indicate if the shape is present in sequence $$x_i$$ or not.  Then you choose some sort of cost functions of the parameters, $$C(K,\beta)$$, which encourages $$\Pr(y | x_i)$$ to match $$y_i$$.
The standard choice is the cross-entropy.  We then set

$$
\begin{align}
  K, \beta:= \arg\min C
\end{align}
$$

There's actually a lot to training, but let's not get side-tracked.  Let me just say, if you understood this section, good for you.  Just don't get cocky.

## Deep CNNs
What was built in the last section was a single layer feed forward CNN.  A deep CNN does what the single layer does, but iterates on the output a few more times, with different kernels.

Why would you do this? I'm at a loss to come up with a concrete example without using too much space, but the basic reason is similar to the reason why [multiple layers are needed for a neural network to learn the XOR gate](https://medium.com/@jayeshbahire/the-xor-problem-in-neural-networks-50006411840b). At a high level, it's quite common for the solution to involve interactions between some sort of atomic units.  For example, if the signal was Morse code, then you might have one detector just for finding the dots and the dashes, and another layer that translates the dots and dashes into letters, then another detector that takes the letters and forms, syllables, then words, etc. Sorry to be so abstract, the reason for multiple layers is intuitively clear to me, but it's still mathematically vague to be honest.

Let's get onto the math. First, we will replace the $$\max$$ operation of the previous section with something less reductive. Instead of reducing a signal down to a single number, we will use max-pooling, which just means dividing the signal into equally spaced bins, perhaps of size $$w > 1$$, and then taking the max in each bin to get a new signal of size $$N / w$$.  We will denote this operation by $${\rm maxpool_w}$$.  Now, if we have a kernel $$K$$, and a series $$x$$, the expression

$$
\begin{align}
  f_1(x) := {\rm maxpool}_{w_1}({\rm ReLU}( [K_1 * x])+\beta_{1})
\end{align}
$$

takes in a signal $$x$$, and outputs a down-sampled signal.  We can then pass this downsampled signal to another function of the same sort, $$f_2$$, with another kernel $$K_2$$ that takes in the series $$f_1(x)$$ as an input and outputs another series, ... then $$f_3$$, you get the point. As a formula we could iteratively define the functions

$$
  \begin{align}
  f_n(x) := {\rm maxpool}_{w_n}( {\rm ReLU}([K_{n} * f_{n-1}(x)]) + \beta_{n})
  \end{align}
$$

for $$n=1,\dots,N$$, and let $$f_0(x) = x$$.
The function $$f_N(x)$$ is an instance of a $$N$-layered CNN.

## The standard CNN architecture
The standard CNN architecture is not just a stack of convolutional layers. Typically the final layers are of a different sort.  If it's classification, the most direct approach is to do $$N$$ convolutional layers and allow subsequent layers to be dense layers. Dense layers are ones that apply an operation such as

$$
\begin{align}
  x_{out} = {\rm ReLU}( \beta_1 \cdot x_{in} + \beta_0)
\end{align}
$$
for a (generically) dense matrix $$\beta_1$$ and vector $$\beta_0$$.

In the final layer we apply the [softmax function](https://en.wikipedia.org/wiki/Softmax_function) so that the output can be interpreted as a probability across a finite set of labels.

## Conclusion
So obviously this blog post does not help so much in regards to implementation.  That's another can of worms.  However, I hope if you are like me, and find lectures drawing those crazy circle and arrow computation diagrams... you know like this one here
<p><a href="https://commons.wikimedia.org/wiki/File:Colored_neural_network.svg#/media/File:Colored_neural_network.svg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/1200px-Colored_neural_network.svg.png" alt="Colored neural network.svg"></a><br>By <a href="//commons.wikimedia.org/wiki/User_talk:Glosser.ca" title="User talk:Glosser.ca">Glosser.ca</a> - <span class="int-own-work" lang="en">Own work</span>, Derivative of <a href="//commons.wikimedia.org/wiki/File:Artificial_neural_network.svg" title="File:Artificial neural network.svg">File:Artificial neural network.svg</a>, <a href="https://creativecommons.org/licenses/by-sa/3.0" title="Creative Commons Attribution-Share Alike 3.0">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=24913461">Link</a></p>

and think to yourself "I feel like I'm watching a shaman interpret the future in a pile of pig entrails", then I hope this post was useful for you.

## Footnotes
